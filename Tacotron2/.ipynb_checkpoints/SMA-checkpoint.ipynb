{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6627771-5de6-43ad-a021-60ac2872b980",
   "metadata": {},
   "source": [
    "## Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS (SMA attention)\n",
    "\n",
    "### Abstract\n",
    "\n",
    "Neural TTS 在生成高质量自然的类人语音方面已经展示出了强大的能力，但是泛化到域外文本上，关于设计基于 attention 的序列到序列的声学模型仍然是一个挑战。在未看过的上下文本的输入中，各种错误都会发生，包括 attention collaps, skipping ,重复等等，这些错误限制了更广泛的应用。在本文中，我们在 seq2seq acoustic model 中提出了一个新颖的 stepwise monotonic attention 方法，它提升对于域外输入的鲁棒性。seq2seq的模型的语音合成系统为当今的主流模式，其attention模块决定了输入和输出的对齐质量，从而影响合成的语音好坏，尤其存在skipping,repeating and attention collapse的问题。这个方法 hard attention 利用了TTS中严格单调的特性, inputs 和 outputs序列的 alignments 不仅要单调而且不允许有跳跃。Soft attention 可以用来避免在推理和训练中不匹配的问题。实验结果表明，提出的方法在域内测试没有任何回归的情况下，基于音素模型的域外情景上鲁棒性得到了提升。\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "近年来，end-to-end neural TTS 方法越来越活跃，不仅在简易性上表现出了重要的优势，而且在产生可以和人类媲美的高度自然的语音方面上表现出了强大的能力。典型的TTS模型如Tacotron2 用 an attention-based seq2seq model，将 characters 或者 phonemes 作为输入并产生输出声学特征序列（如 梅尔频谱图）来自高质量的原始 waveform可以由 neural vocoder生成，如 WaveNet.\n",
    "\n",
    "然而，目前的方法虽然在与训练语料库相似的输入时，总体上结果令人满意，但是在域外输入上还不够鲁棒，尤其是在长时间复杂的或者异常发音的情况下：不能接受输入脚本发音错误的情况发生，经常会 skipping, repeating 或者 attention collapse (当模型不能集中于 a single input token时会出现难以理解的胡言乱语)。如此鲁棒性的问题导致更广泛的使用 neural TTS 存在重大的障碍，当模型的输入更加多样化和不可控时，解决这个问题具有很大的价值。\n",
    "\n",
    "通常此类问题可归因于在 attention mechanism 中发生了 misalignments 情况，有望在输入和输出中预测出共同适用于 encoder 和 decoder 的 alignments. 也就是说，确定在每个输出步骤中说出相应的 input token. 并对语音中的持续时间和停顿进行潜在建模。因此，我们强调把 attention mechanism 的鲁棒性作为我们最关心的问题，开始我们的观察：人类以原始顺序读出音素，每个音素都被读出并有助于产生一段语音片段，不像语音识别，连续输入帧(e.g. 一段静音片段)对输出没有影响。因此遵循这种严格的标准，alignment 应该是从 output frame 到 input token 的 surfective mapping:(1)Locality: 输出帧都能映射到相应的输入，这样可以避免 attention collapse;(2) Monotonicity: the position  of  the  aligned  input  token  must  never  rewind backward(倒退), which prevents repeating;（3)Completence:每个input token必须被cover 或者与某些output frame 对齐，避免skipping。\n",
    "\n",
    "为了确保正确对齐而提出的各种 attention mechanisms 在都满足这三个标准上总是失败。特别的，TTS的完整性没有彻底讨论。在本文中，我们调查了在neural TTS models中的 attention mechanism 并提出了一个新的方法，stepwise monotonic attention. 我们的方法是基于 monotonic attention, 为了满足上面的三个标准，对于完整性我们强制进行了额外的限制。实验表明。即使给出最奇怪的输入我们的方法也非常稳定，与此同时，保证了自然度对标在域内句子上最先进的模型。\n",
    "\n",
    "### 2. Related work\n",
    "\n",
    "#### 2.1 Attention-based neural TTS\n",
    "\n",
    "通常情况下，在neural TTS上的声学模型是由 attention-based encoder-decoder model 完成的。Encoder outputs $x=[x_1,x_2,...,x_n]$ 作为 'memory' entries, 在每一个时间步$i$, 'energy' 值 $e_{i,j}$ is evaluated for each $x_j$  by a trainable  attention  mechanism,  given  the  previous decoder hidden state $h_{i-1}$. \n",
    "\n",
    "$$e_{i,j}=\\text{Attention}(h_{i-1},x_j)\\tag{1}$$\n",
    "\n",
    "$$\\alpha_{i,j}=\\frac{\\text{exp}(e_{i,j})}{\\sum_{k=1}^n \\text{exp}(e_i, k)}=\\text{softmax}(e_{i,j})\\tag{2}$$\n",
    "\n",
    "$$c_i = \\sum_{j=1}^n \\alpha_{i,j} x_j\\tag{3}$$\n",
    "\n",
    "其中 $\\alpha_i$ 是 an alignment vector, $c_i$ 是生成的context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23381d3b-1d09-41cb-bc6a-3ee47e41406e",
   "metadata": {},
   "source": [
    "最近，attention mechanisms 出现了许多变体，其中一些专门为局部性(locality)和单调性(monotonicity), 针对locality,提出类似[5-8]的方法，而单调性和完整性可以通过显式建模对齐的方法进一步改进，over  memories  with  non-decreasing  position[9-12]。其他方法通过惩罚非对角对齐来鼓励单调性和完整性，或者利用之前在Tacotron2中的 location sensitive attention 来对齐。特别地，在TTS中的 forward attention 被提出用来稳定 attention过程，which  reweighs(重新衡量)  the alignments by previous ones using forward variables, possibly modulated(调制) by a “transition agent(过渡剂)” [14].  \n",
    "\n",
    "#### 2.2 Monotonic attention\n",
    "\n",
    "在所有的 attention 变体中，monotonic attention 被证明对于严格单调性和局部性上是特别有效的，并已应用到包括TTS在内的多个任务中。这个机制如下：对于每一步 $i$，the mechianism 在 memory index(索引) $t_{i-1}$ 中检查 memory entries, 它集中在前一步。根据公式(1)计算 energy value $e_{i,j}$, but a “selection probability” $p_{i,j}$, is evaluated by logistic sigmoid function instead: \n",
    "$$p_{i,j}=\\sigma(e_{i,j})\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385475ec-d528-4114-9eaa-dad141f57514",
   "metadata": {},
   "source": [
    "从 $j=t_{i-1}$开始，at each time the mechanism 将根据 Bernoulli来判断是否对$j$(memory entry)进行前移，直到输入末端，或者直到收到正样本 $z_{ij}=1$，$j$将保持前移，当 $j$停下来，则memory $x_j$ 将直接被选为 $c_i$. 有了这样的限制，保证了每一步只关注一个entry 并且他的位置永远不会倒退。此外，the mechanism 只需要线性时间复杂度并支持在想输入。这在实践中是有效的。\n",
    "\n",
    "A memory entry $c_i$ 的这种具有不可微的选择机制被视为‘hard’ attention，它在以一种激进的方式确保句不行，相反的 'soft' attention 则用了连续权重。因此，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e26ec-4505-4e57-b8e4-8b0ebddc0d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
